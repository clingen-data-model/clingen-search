* On data import, there is no typechecking when a property should have a resource as its object, but instead recieves a literal

This was noticed as a part of the malfunctioning gene / load triples script. The resource expected as a target a gene IRI, and instead imported a string literal with the Gene IRI. This error was corrected, but leaves an issue where object properties may instead return a literal.

** Suggested resolution:

Properties in OWL have either a Data Property or a Object Property type. Ensure on import that these types are used to validate the object of their referent.

Alternatively, use these properties to determine the type of the referrent imported. Throw an error when type does not align as expected.

* There may be classes and properties that aren't namespaced appropriately in local-names

When the base is imported, a check should be made for any un-namespaced keywords in local-names, these should be at least pointed out.

For the time being, (2019-02-08) all known classes are covered

* Implement bi-directional traversal of names
* Complete datafy/nav implementation

  Being able to use the Cognitect REBL would be huge, need a substitute for the browser in Neo4j.

* Need appropriate migration path when class and property labels change in OWL file

Since local names in the code are based on labels defined in the base ontology and RDF files, there should exist code that can do the following steps:

As an alternative, there should be code that can retain the old names. In this sense, the local-names hashes would be stored in the code base as configuration files; names woudld be added to iteratively (but not replaced altogether). This approach may be easier as a first pass.

With the current implementation, name translation is not entirely feasible to use for loading data. Property names should be fine as long as one can guarantee that the OWL files have been loaded, but a lot of class name shortcuts won't exist since instances of them won't exist yet. There is also the question of updating the data whenever these instances are created.

It seems to make the most sense to generate the symbol->iri table and load it separately from the database. There can be a pathway to update with new ontologies and files, but this makes the most sense for now. Should require no change to code outside of database.names.

Where do the update methods belong? Does it make sense to create an admin namespace for these sorts of operations? (yes)

** Detect when a name has changed

** Identify instances of that name in queries and code

** Automatically (or interactively) update the name to the new iteration


* Admin operations
** Refresh names

A fully robust implementation of this should be able to:

*** Identify relevant names in the database
Done, existing queries support this

*** Write names in edn format if they don't already exist

*** If they do, read existing names and compare

Perhaps report the disparity and abort unless a flag is set. User is encouraged to run the command once without the flag, and then set the flag later

*** Beyond MVP: automatically detect instances of changed names used in the code.

*** Way beyond MVP: automatically update them.

* Check for class names that have no label

"select distinct ?x where { [] a ?x }" returns 42, there are 37 entries in class-names, some classes are likely unlabeled



* Replace symbol in query should cover case where symbol is not terminated by space

For example:

clingen-search.source.html.elements> (q/register-query ::genes "select ?x where {?x a :so/Gene} limit 5")
Execution error (QueryParseException) at org.apache.jena.sparql.lang.ParserBase/throwParseException (ParserBase.java:522).
Line 1, column 23: Unresolved prefixed name: :so
clingen-search.source.html.elements> (q/register-query ::genes "select ?x where { ?x a :so/Gene } limit 5")
true
* Consider what to do when a resource has multiple types in dispatch of sink/html/elements
* Consider implications of OWL subclassing/inheritance in dispatch of sink/html/elements

In particular, is it possible to represent elements based on their more specific classes when available, and more general classes when not. Would be lovely to create a construct based on the ad-hoc subtyping available in the multimethod intervace.
* Implement interface to RDFResource that always returns properties as a collection

Being able to access properties as if a resource were simply a hash is super slick. Sometimes one wants to iterate over properties as if they were guaranteed to be a collection, along the lines of the xml-> and xml1-> zipper syntax.

Maybe the solution is to bring the zipper interface to RDFResource.

* Post-processing to consider:

** Labels

Probaby want to be consistent about the label property we want to use for terms. :skos/preferred-label feels correct, may want to translate that to :rdfs/label instead.

Could easily be satisfied by a CONSTRUCT query, particularly if one is permitted to keep the original label.

** Diseases

*** Translation to MONDO

The labels and class hierarchies are all tied to MONDO terms, but gene dosage records are tied to OMIM terms, may want to make sure that all disease terms are translated into MONDO terms.

Here we need to delete the original association, this should be a 1-1 relation.

*** Typing

Diseases should have a type assigned to them to mark them as diseases.

* Datafy

** CURIE/IRI of disease nil in one case

I got the following output when looking up aneurysm-osteoarthritis syndrome, the IRI/CURIE is nil in one case

clingen-search.database.query> (-> g (get [:geno/is-feature-affected-by :<]) first (get [:sepio/has-subject :<]) :sepio/has-object (get [:skos/has-exact-match :<]) datafy)
{:>
 {nil "MONDO:0013426",
  :rdf/type http://www.w3.org/2002/07/owl#Class,
  :oboInOwl/has-exact-synonym "Loeys-Dietz syndrome type 3",
  :oboInOwl/has-related-synonym
  "Loeys-Dietz syndrome, type 1C (formerly)",
  :oboInOwl/database-cross-reference "UMLS:C3151087",
  :rdfs/label "aneurysm-osteoarthritis syndrome",
  :owl/equivalent-class http://purl.obolibrary.org/obo/OMIM_613795,
  :skos/has-exact-match http://purl.obolibrary.org/obo/DOID_0070237,
  :oboInOwl/in-subset http://purl.obolibrary.org/obo/mondo#clingen,
  :rdfs/sub-class-of ad56daa9-3262-437b-be73-54bfe39680cd},
 :<
 #:owl{:annotated-source f5f3fb8d-bc35-451d-8193-b1ed07e6c9b5,
       :equivalent-class http://purl.obolibrary.org/obo/GARD_0010997}}

* Performance

** Paths for local resources require an O(n) search through namespaces to generate

Am currently using curie, blasting through the lists of namespaces. There should be a better way to handle this
* Loading dependency inversions

  There are a couple interfaces with dependency inversions, the datafy implementaiton of Jena resources, and the pages themselves. May need to require these explicity, but it would be nice if that wasn't necessary
* Creation and management of Clojure class hierarchy
* define 'server ready' criteria

Kubernettes supports querying a pod for 'readiness'; when a server is considered to be in a usable state (distinct from a failed or unhealthy state). We should implement these as endpoints in pedestal.

Probably this logic lives in or close to clingen-search.service

** Kafka streams are close enough to up to date

Technically a stream is never 'finished', but we should not register as ready until the messages that were available at app startup are consumed.

** Base data is loaded

Base data should be completely loaded. Realistically this needs to happen prior to starting any stream consumption.

* Exceptions when loading and transforming data

Currently we're coding too much along the 'happy path'. Either we need to reprogram transformations to be tolerant in all cases of missing data (or to return nil when it's not possible to run a transformation due to missing data), or we need to wrap transformations with exception handling. Probably both, though dealing with unmanagable data is a problem that is yet to be addressed.

* Time sequence when loading data

Should we load all data in the sequence it was recieved in (as the NY Times does with their monologue), or is there a sequence that should be insisted on. This is key with the base data we're working with.
