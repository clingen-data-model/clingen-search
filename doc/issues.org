* On data import, there is no typechecking when a property should have a resource as its object, but instead recieves a literal

Priority LOW: probably SHACL serves any purpose required for validation and it's not worth performing this check for now

This was noticed as a part of the malfunctioning gene / load triples script. The resource expected as a target a gene IRI, and instead imported a string literal with the Gene IRI. This error was corrected, but leaves an issue where object properties may instead return a literal.

** Suggested resolution:

Properties in OWL have either a Data Property or a Object Property type. Ensure on import that these types are used to validate the object of their referent.

Alternatively, use these properties to determine the type of the referrent imported. Throw an error when type does not align as expected.

* Need appropriate migration path when class and property labels change in OWL file

Priority LOW: currenty practice is to set all labels manually, including name changes

Since local names in the code are based on labels defined in the base ontology and RDF files, there should exist code that can do the following steps:

As an alternative, there should be code that can retain the old names. In this sense, the local-names hashes would be stored in the code base as configuration files; names woudld be added to iteratively (but not replaced altogether). This approach may be easier as a first pass.

With the current implementation, name translation is not entirely feasible to use for loading data. Property names should be fine as long as one can guarantee that the OWL files have been loaded, but a lot of class name shortcuts won't exist since instances of them won't exist yet. There is also the question of updating the data whenever these instances are created.

It seems to make the most sense to generate the symbol->iri table and load it separately from the database. There can be a pathway to update with new ontologies and files, but this makes the most sense for now. Should require no change to code outside of database.names.

Where do the update methods belong? Does it make sense to create an admin namespace for these sorts of operations? (yes)

** Detect when a name has changed

** Identify instances of that name in queries and code

** Automatically (or interactively) update the name to the new iteration
*** Identify relevant names in the database
Done, existing queries support this

*** Write names in edn format if they don't already exist

*** If they do, read existing names and compare

Perhaps report the disparity and abort unless a flag is set. User is encouraged to run the command once without the flag, and then set the flag later

*** Beyond MVP: automatically detect instances of changed names used in the code.

*** Way beyond MVP: automatically update them.



* Post-processing to consider:

** Labels

Probaby want to be consistent about the label property we want to use for terms. :skos/preferred-label feels correct, may want to translate that to :rdfs/label instead.

Could easily be satisfied by a CONSTRUCT query, particularly if one is permitted to keep the original label.

** Diseases

*** Translation to MONDO

The labels and class hierarchies are all tied to MONDO terms, but gene dosage records are tied to OMIM terms, may want to make sure that all disease terms are translated into MONDO terms.

Here we need to delete the original association, this should be a 1-1 relation.

*** Typing

Diseases should have a type assigned to them to mark them as diseases.

* Datafy

** CURIE/IRI of disease nil in one case

I got the following output when looking up aneurysm-osteoarthritis syndrome, the IRI/CURIE is nil in one case

clingen-search.database.query> (-> g (get [:geno/is-feature-affected-by :<]) first (get [:sepio/has-subject :<]) :sepio/has-object (get [:skos/has-exact-match :<]) datafy)
{:>
 {nil "MONDO:0013426",
  :rdf/type http://www.w3.org/2002/07/owl#Class,
  :oboInOwl/has-exact-synonym "Loeys-Dietz syndrome type 3",
  :oboInOwl/has-related-synonym
  "Loeys-Dietz syndrome, type 1C (formerly)",
  :oboInOwl/database-cross-reference "UMLS:C3151087",
  :rdfs/label "aneurysm-osteoarthritis syndrome",
  :owl/equivalent-class http://purl.obolibrary.org/obo/OMIM_613795,
  :skos/has-exact-match http://purl.obolibrary.org/obo/DOID_0070237,
  :oboInOwl/in-subset http://purl.obolibrary.org/obo/mondo#clingen,
  :rdfs/sub-class-of ad56daa9-3262-437b-be73-54bfe39680cd},
 :<
 #:owl{:annotated-source f5f3fb8d-bc35-451d-8193-b1ed07e6c9b5,
       :equivalent-class http://purl.obolibrary.org/obo/GARD_0010997}}

* Performance

** Paths for local resources require an O(n) search through namespaces to generate

Am currently using curie, blasting through the lists of namespaces. There should be a better way to handle this
* Loading dependency inversions

  There are a couple interfaces with dependency inversions, the datafy implementaiton of Jena resources, and the pages themselves. May need to require these explicity, but it would be nice if that wasn't necessary
* Creation and management of Clojure class hierarchy
* define 'server ready' criteria

Kubernettes supports querying a pod for 'readiness'; when a server is considered to be in a usable state (distinct from a failed or unhealthy state). We should implement these as endpoints in pedestal.

Probably this logic lives in or close to clingen-search.service

** Kafka streams are close enough to up to date

Technically a stream is never 'finished', but we should not register as ready until the messages that were available at app startup are consumed.

** Base data is loaded

Base data should be completely loaded. Realistically this needs to happen prior to starting any stream consumption.

* Exceptions when loading and transforming data

Currently we're coding too much along the 'happy path'. Either we need to reprogram transformations to be tolerant in all cases of missing data (or to return nil when it's not possible to run a transformation due to missing data), or we need to wrap transformations with exception handling. Probably both, though dealing with unmanagable data is a problem that is yet to be addressed.

* Time sequence when loading data

Should we load all data in the sequence it was recieved in (as the NY Times does with their monologue), or is there a sequence that should be insisted on. This is key with the base data we're working with.
* Returning meaningful results when something is 'not found' in graphql
  
Right now graphql queries are doing no checking to make sure an entity exists of the type that's being requested. Despite the fact that anything (even a resource not in the triplestore) can be represented as a resource, most anything without an explicit type, and certainly without any statements, is unlikely to be a usable concept and should get the graphql equivalent of a 404
* Database 'migrations'

The data service doesn't really have in-place manipulation of data, per-se. Rather, when the structure of the database changes, the input scripts are modified to target the new structure, and the input data is reprocessed according to the new code.

There is currently no mechanism to trigger this. A naive (and possibly completely adequate approach) would be to have a numeric migration version as part of a deploy. Whenever this number advances, all deployments blow away their local data and recreate it all from scratch. This process can happen as part of a new k8s rollout, so that availability is not affected. Liveness and readiness monitoring will need to be implemented for this to work properly.

A more sophisticated approach would specify which resources have changed, and update only those resources. There may be resources downstream of that change which may also need to be updated. Implementing a resource dependency tree would be necessary for this approach to function correctly. This may be desirable for other reasons.

* Incoming data validation and shapes

The data service relies on incoming data from a variety of external services. This data may come across the wire in different serializations: JSON, CSV, and various flavors of RDF. Both JSON and RDF have techniques and libraries that can be used to describe the expected 'shape' of incoming data, between JSON Schema, SHACL, and even Clojure spec for data that parses into Clojure data types.

Passing incoming data through these validators may help tremendously with identifying and diagnosing issues as they come up. These may be either baked into the data service, or constructed in such a way that they can be deployed as independent services (in the stream of data being passed through Kafka, for instance).

Regardless, these should be written in such a way that they can be separated from the graphql service later. There is also the question of how to report and log validation errors.

* Factoring out modules and libraries

Right now everything is being built all-together in the same project, largely because requirements for loading and querying data are still resulting in significant changes to the core query interface. This will have to change somewhat when the query interface is used in other contexts. It would also be nice to factor it for clojurescript as well at some point.

* Multithreading

Based on the lacinia documentation, the service is configured to run on a single service resolving thread. The service should be tested and changed so that multiple threads can be used.

* Transactions

It would be best to wrap a response in a single transaction to guarantee consistency of data. Right now each data access opens its own transaction.

